{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7g9lJWv/a7pXS5VTODquS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratyushplp/CryptoAnalysis/blob/main/RLAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a program in Python to implement value iteration, policy iteration, and modified policy iteration\n",
        "specifically for this simple MDP example.\n",
        "For this matter, you should start by creating a simple MDP class using class MDP. This class should\n",
        "include the following members:\n",
        "‚ñ™ a constructor for the MDP class def __init()__ that has the following parameters: self,\n",
        "T, R, discount. \n",
        "o T -- Transition function: |A| x |S| x |S'| array\n",
        "o R -- Reward function: |A| x |S| array\n",
        "o discount -- discount factor Œ≥: scalar in [0,1)\n",
        "The constructor should verify that the inputs are valid (using the assert command) and set\n",
        "corresponding variables in an MDP object.\n",
        "‚ñ™ a procedure for the value iteration def valueIteration() that has the following\n",
        "parameters: self, initialV , nIterations, tolerance.\n",
        "Set nIterations and tolerance to np.inf and 0.01 as default values, respectively.\n",
        "o initialV -- Initial value function: array of |S| entries\n",
        "o nIterations -- limit on the number of iterations: scalar (default: infinity)\n",
        "o tolerance -- threshold on ‚Äñùëâùëõ ‚àí ùëâùëõ+1‚Äñ‚àû that will be compared to a variable epsilon\n",
        "(initialized to np.inf): scalar (default: 0.01)\n",
        "This procedure should return a new value function V.\n",
        "o newV ‚Äì New value function: array of |S| entries.\n",
        "o iteration ‚Äì the number of iterations performed: scalar\n",
        "o epsilon -- ‚Äñùëâùëõ ‚àí ùëâùëõ+1‚Äñ‚àû: scalar"
      ],
      "metadata": {
        "id": "fwk_cjaTyUDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#AUTHOR: PRATYUSH PRADHAN\n",
        "#imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n"
      ],
      "metadata": {
        "id": "ZlQbPNg91mfv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rough\n",
        "temp= np.array([[1,2,3],[4,5,6]])\n",
        "isinstance(temp, np.ndarray)\n",
        "print(temp.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6F950spC12ra",
        "outputId": "e6b9c07d-a3b4-489f-df72-c43eb0316026"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "r8FE-OJIxIMc"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MDP:\n",
        "  def __init__(self,T,R,discount):\n",
        "    assert isinstance(T, np.ndarray) and isinstance(R, np.ndarray), \"The transition and reward array should be numpy arrays\"\n",
        "    assert discount >0 and discount <= 1, \"The discount factor should be scalar in [0,1)\"\n",
        "    assert (R.shape[0] == T.shape[1] and T.shape[1] == T.shape[2]), \"Invalid shape of input array\"#dimension check for R and T\n",
        "    self.T = T # Transition probability T is a 3 dimensional matrix which contains both TA and TS\n",
        "    self.R = R\n",
        "    self.discount = discount\n",
        "  \n",
        "  def valueFunction(self, oldV,getMax=False):\n",
        "    newV = self.R + self.discount*np.dot(self.T,oldV) #Belmans equation\n",
        "    policy=np.argmax(newV,axis=0)\n",
        "    VA,VS= newV\n",
        "    if(not getMax):\n",
        "      return newV\n",
        "    maxV= np.maximum(VA,VS)\n",
        "    return newV,maxV\n",
        "\n",
        "  #NOTE: The policy determines which transition matrix (for probability distribution) should be taken.\n",
        "  #for each state we have action A and S. The transition probability for the policy will depend\n",
        "  # on the action we take on that state. T=> for a state T[A] or T[S]?\n",
        "  def getTransitionForPolicy(self,policy):\n",
        "    newT = []\n",
        "    for ind,item in enumerate(policy):\n",
        "      newT.append(self.T[item[0]][ind]) #chosing transition matrix according to policy\n",
        "    newT=np.array(newT)\n",
        "    return newT;\n",
        "  \n",
        "  def valueIteration( self, initialV , nIterations = np.inf, tolerance = 0.01):\n",
        "    assert isinstance(initialV, np.ndarray), \"The intial value function array should be numpy arrays\"\n",
        "    assert self.R.shape[0] == initialV.shape[0], \"Invalid shape of input array\"\n",
        "\n",
        "    n = 0\n",
        "    epsilon = float('inf')\n",
        "    oldV = initialV\n",
        "    maxV= None\n",
        "    newV= None\n",
        "    while(n< nIterations and epsilon > tolerance):\n",
        "      newV,maxV= self.valueFunction(oldV,True)\n",
        "      epsilon = np.linalg.norm(maxV - oldV)\n",
        "      oldV=maxV\n",
        "      n+=1\n",
        "      #print(f'iteration {n} \\n value iteration inside loop new value : {newV} \\n, max value: {maxV} \\n, epsilon: {epsilon}')\n",
        "    print(f'Final Result of Value iteration \\n No of iteration :{n} \\n Final Valuefunction : {newV} \\n, max value: {maxV} \\n, epsilon: {epsilon}')\n",
        "    return newV,n,epsilon\n",
        "  \n",
        "  def extractPolicy(self, V):\n",
        "    policy=np.argmax(V,axis=0)\n",
        "    #NOTE: 0 represents A and 1 represents S\n",
        "    return policy \n",
        "\n",
        "  def evaluatePolicy(self, policy):\n",
        "    #MAIN FORMULA : V = R + gamma*T*V, V= R*(I-gamma*T))^-1\n",
        "    newT = self.getTransitionForPolicy(policy)\n",
        "    I = np.identity(self.T[0].shape[0])  #The identity matrix should be the shape as V. The V will have same shape as R. But as we need to substract the matrix with T\n",
        "    tempInv = np.linalg.inv((I-self.discount*newT))\n",
        "    newV = np.dot(tempInv,self.R)\n",
        "    return newV\n",
        "\n",
        "  def improvePolicy(self, oldV):\n",
        "    newV= self.valueFunction(oldV)\n",
        "    policy = self.extractPolicy(newV)\n",
        "    return policy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #2 steps, 1) evaluate policy 2) improve policy    \n",
        "  def policyIteration(self, initialPolicy , nIterations= np.inf):\n",
        "    assert isinstance(initialPolicy, np.ndarray), \"The initial policy should be numpy arrays\"\n",
        "    print(f'The initial policy is {initialPolicy}')\n",
        "    n=0\n",
        "    policy= initialPolicy\n",
        "    newPolicy= None\n",
        "    while n < nIterations:\n",
        "      V = self.evaluatePolicy(policy)\n",
        "      newPolicy = self.improvePolicy(V)\n",
        "      # print(f'iteration {n} \\n policy iteration inside loop new value : {V} \\n,new policy: {newPolicy} \\n')\n",
        "      n+=1\n",
        "      if(np.array_equal(policy,newPolicy)):#Convergence Criteria\n",
        "        break\n",
        "      policy = newPolicy\n",
        "\n",
        "\n",
        "    print(f'Final Result of Policy iteration \\n No of iteration :{n} \\n Final Valuefunction : {V} \\n, Final Policy :{newPolicy}')\n",
        "    return n,newPolicy\n",
        "\n",
        "\n",
        "#NOTE: The goal of partial policy evaluation is to estimate the value function of a fixed policy, without changing the policy itself.(i.e the policy remains constant) \n",
        "  def evaluatePolicyPartially(self,policy,initialV,nIterations = np.inf,tolerance=0.01,isReturnEpsilon = False):\n",
        "    newT = self.getTransitionForPolicy(policy)\n",
        "    n = 0\n",
        "    epsilon = float('inf')\n",
        "    V=initialV\n",
        "    while(n<nIterations and epsilon > tolerance):\n",
        "      newV = self.R + self.discount*(np.dot(newT,V))\n",
        "      epsilon = np.linalg.norm(newV - V)\n",
        "      #print(f'YOLO Epsilon {epsilon} newValue {newV} inside iteration {n}')\n",
        "      V= newV\n",
        "      n+=1\n",
        "    #print(f'The value function after {n} iteration = V \\n Epsilon = {epsilon}')\n",
        "    if( not isReturnEpsilon):\n",
        "      return V\n",
        "    return V,epsilon\n",
        "\n",
        "\n",
        "\n",
        "  def modifiedPolicyIteration(self,initialPolicy,initialV,nEvalIterations=5,nIterations = np.inf,tolerance=0.01):\n",
        "    #main point: performs approximate policy evaluation using value iteration and then performs approximate policy improvement to obtain a better policy based on the approximate value function. \n",
        "    #keyword APPROXIMATE\n",
        "    policy=initialPolicy\n",
        "    V= np.zeros(self.R.shape)\n",
        "    oldV=V\n",
        "    n=0\n",
        "    newV = None\n",
        "    epsilon = float('inf')\n",
        "    while(n<nIterations and epsilon>tolerance):\n",
        "      #Question: If epsilon continues to be less than 0.01, but the policy are not same, the the value of V wont change?\n",
        "      newV,epsilon = self.evaluatePolicyPartially(policy,oldV,nEvalIterations,tolerance,True)\n",
        "      newPolicy = self.improvePolicy(newV)\n",
        "      epsilon = np.linalg.norm(newV - oldV) #Convergence Criteria\n",
        "      #ask\n",
        "      oldV = newV\n",
        "      policy = newPolicy\n",
        "      n+=1\n",
        "    return policy,n,epsilon\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "\n",
        "#initialization\n",
        "T_A = np.array([[0.5,0.5,0,0],[0,1,0,0],[0.5,0.5,0,0],[0,1,0,0]])\n",
        "T_S = np.array([[1,0,0,0],[0.5,0,0,0.5],[0.5,0,0.5,0],[0,0,0.5,0.5]])\n",
        "T = np.array([T_A, T_S])\n",
        "discount=0.9\n",
        "R = np.array([[0],[0],[10],[10]])\n",
        "initalV = np.zeros(R.shape)\n",
        "mdp = MDP(T,R,discount)\n",
        "\n",
        "#for policy iteration initial policy pi(PU) = A pi(PF)= A pi(RU)=A pi(RF)=A\n",
        "initialPolicy= np.array([[0],[0],[0],[0]])"
      ],
      "metadata": {
        "id": "yCn5HLgKYTGn"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#functions\n",
        "newV,n,epsilon=mdp.valueIteration(initalV,80,0.01)\n",
        "policy= mdp.extractPolicy(newV)\n",
        "print(f'Optimal policy for value iteration = {policy}')\n",
        "\n",
        "mdp.policyIteration(initialPolicy,50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7EyJHmp7ZPg",
        "outputId": "ba7a2dc0-7526-449b-f094-d2268d0e5642"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Result of Value iteration \n",
            " No of iteration :65 \n",
            " Final Valuefunction : [[[31.54265964]\n",
            "  [34.70117008]\n",
            "  [41.54265964]\n",
            "  [44.70117008]]\n",
            "\n",
            " [[28.38414921]\n",
            "  [38.56157171]\n",
            "  [43.98173159]\n",
            "  [54.15915409]]] \n",
            ", max value: [[31.54265964]\n",
            " [38.56157171]\n",
            " [43.98173159]\n",
            " [54.15915409]] \n",
            ", epsilon: 0.009432147662192136\n",
            "Optimal policy for value iteration = [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "The initial policy is [[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "Final Result of Policy iteration \n",
            " No of iteration :2 \n",
            " Final Valuefunction : [[31.58510431]\n",
            " [38.60401638]\n",
            " [44.02417625]\n",
            " [54.20159875]] \n",
            ", Final Policy :[[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, array([[0],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1]]))"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Questions\n",
        "#1. Report the policy, value function, and the number of iterations needed by value iteration when\n",
        "# using a tolerance of 0.01 and starting from a value function set to 0 for all states\n",
        "initialV = np.zeros(R.shape)\n",
        "newV,n,epsilon = mdp.valueIteration(initialV,80,0.01)\n",
        "policy= mdp.extractPolicy(newV)\n",
        "print(f'Optimal policy for value iteration = {policy}')\n",
        "print('**********')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUlRabTdXOTT",
        "outputId": "a676b7a5-e65c-4bd9-f4e5-c31311d3facb"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Result of Value iteration \n",
            " No of iteration :65 \n",
            " Final Valuefunction : [[[31.54265964]\n",
            "  [34.70117008]\n",
            "  [41.54265964]\n",
            "  [44.70117008]]\n",
            "\n",
            " [[28.38414921]\n",
            "  [38.56157171]\n",
            "  [43.98173159]\n",
            "  [54.15915409]]] \n",
            ", max value: [[31.54265964]\n",
            " [38.56157171]\n",
            " [43.98173159]\n",
            " [54.15915409]] \n",
            ", epsilon: 0.009432147662192136\n",
            "Optimal policy for value iteration = [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "**********\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 2\n",
        "#Report the policy, value function, and the number of iterations needed by policy iteration to  find an optimal policy when starting from the policy \n",
        "#that chooses action 0 in all states. Note: action 0 corresponds to ‚ÄúA: Advertising‚Äù whereas action 1 corresponds to ‚ÄúS: Saving money‚Äù\n",
        "initialPolicy= np.array([[0],[0],[0],[0]])\n",
        "n,newPolicy = mdp.policyIteration(initialPolicy,50)\n",
        "print(f'the final policy iteration  number of iterations ={n} \\n ,optimal policy: {newPolicy}  \\n')\n",
        "print('**********')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z61sSSCfUuS9",
        "outputId": "fcd83a82-a213-4f20-cb74-0dd0024305fe"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The initial policy is [[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "Final Result of Policy iteration \n",
            " No of iteration :2 \n",
            " Final Valuefunction : [[31.58510431]\n",
            " [38.60401638]\n",
            " [44.02417625]\n",
            " [54.20159875]] \n",
            ", Final Policy :[[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "the final policy iteration  number of iterations =2 \n",
            " ,optimal policy: [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]  \n",
            "\n",
            "**********\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q) 3.\tReport the number of iterations needed by modified policy iteration to converge when varying the number of iterations in partial policy evaluation from 1 to 10.\n",
        "#  Use a tolerance of 0.01, start with the policy that chooses action 0 in all states and start with the value function that assigns 0 to all states.\n",
        "for value in range(1,11):\n",
        "  policy,n,epsilon= mdp.modifiedPolicyIteration(initialPolicy,initialV,nEvalIterations=value,nIterations = 100,tolerance=0.01)\n",
        "  print(f'For modified policy iteration with number_of_iteration_in_partial_policy = {value}\\n  number of iterations required to converge ={n} \\n optimal policy: {policy}  \\n')\n",
        "print('**********')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0gteB-JVXbh",
        "outputId": "53e6b4fd-60e9-48a6-d3c2-657e998c5073"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For modified policy iteration with number_of_iteration_in_partial_policy = 1\n",
            "  number of iterations required to converge =65 \n",
            " optimal policy: [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]  \n",
            "\n",
            "For modified policy iteration with number_of_iteration_in_partial_policy = 2\n",
            "  number of iterations required to converge =34 \n",
            " optimal policy: [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]  \n",
            "\n",
            "For modified policy iteration with number_of_iteration_in_partial_policy = 3\n",
            "  number of iterations required to converge =23 \n",
            " optimal policy: [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]  \n",
            "\n",
            "For modified policy iteration with number_of_iteration_in_partial_policy = 4\n",
            "  number of iterations required to converge =18 \n",
            " optimal policy: [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]  \n",
            "\n",
            "For modified policy iteration with number_of_iteration_in_partial_policy = 5\n",
            "  number of iterations required to converge =15 \n",
            " optimal policy: [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]  \n",
            "\n",
            "For modified policy iteration with number_of_iteration_in_partial_policy = 6\n",
            "  number of iterations required to converge =13 \n",
            " optimal policy: [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]  \n",
            "\n",
            "For modified policy iteration with number_of_iteration_in_partial_policy = 7\n",
            "  number of iterations required to converge =11 \n",
            " optimal policy: [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]  \n",
            "\n",
            "For modified policy iteration with number_of_iteration_in_partial_policy = 8\n",
            "  number of iterations required to converge =10 \n",
            " optimal policy: [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]  \n",
            "\n",
            "For modified policy iteration with number_of_iteration_in_partial_policy = 9\n",
            "  number of iterations required to converge =9 \n",
            " optimal policy: [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]  \n",
            "\n",
            "For modified policy iteration with number_of_iteration_in_partial_policy = 10\n",
            "  number of iterations required to converge =9 \n",
            " optimal policy: [[0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]  \n",
            "\n",
            "**********\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.Discuss the impact of the number of iterations in partial policy evaluation on the results and relate the results to value iteration and policy iteration\n",
        "# Ans: As we can see from the results when we use  higher number of iterations (for policy evaluation) it will take less iteration to converge as shown in the snippets"
      ],
      "metadata": {
        "id": "9NgmRRBSWJrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXTRA NOTES:\n",
        "Policy Iteration vs Modified Policy Iteration\n",
        "**bold text**\n",
        "\n",
        "The main difference between normal policy iteration and modified policy iteration lies in how they perform policy evaluation and policy improvement.\n",
        "\n",
        "In normal policy iteration, the algorithm performs exact policy evaluation to determine the value function for a given policy, and then performs exact policy improvement to obtain a better policy based on the value function. This process is repeated until the policy converges to an optimal policy. Exact policy evaluation involves solving a set of linear equations or matrix inversion, which can be computationally expensive, especially for large problems.\n",
        "\n",
        "In modified policy iteration, the algorithm performs approximate policy evaluation using value iteration, and then performs approximate policy improvement to obtain a better policy based on the approximate value function. This process is repeated until the policy converges to an optimal policy. Value iteration involves iteratively updating the value function using the Bellman equation, which is less computationally expensive than exact policy evaluation.\n",
        "\n",
        "The advantage of modified policy iteration over normal policy iteration is that it can converge to an optimal policy faster, especially for large problems, because it avoids the computational expense of exact policy evaluation. However, the policy obtained by modified policy iteration may not be as accurate as the one obtained by normal policy iteration because it relies on approximations.\n",
        "\n",
        "Partial Policy Iteration\n",
        "**bold text**\n",
        "In reinforcement learning, partial policy evaluation refers to the process of estimating the value function of a policy based on incomplete information. Specifically, it involves estimating the value function of a policy for some states, without necessarily having information about the policy for all states.\n",
        "\n",
        "One common approach to partial policy evaluation is to use the Bellman equation, which relates the value function of a state to the value function of its successor states. By recursively applying the Bellman equation, we can estimate the value function for a set of states, even if we do not have complete information about the policy.\n",
        "\n",
        "THE POLICY DURING POLICY ITERATION DOES NOT CHANGE. The goal of partial policy evaluation is to estimate the value function of a fixed policy, without changing the policy itself.\n",
        "\n",
        "\n",
        "Identity Matrix\n",
        "**bold text**\n",
        "An identity matrix is a special type of square matrix in linear algebra that has ones along the diagonal and zeros elsewhere. It is denoted by the symbol \"I\" and has the property that when it is multiplied by any square matrix of the same size, the result is the original matrix.\n",
        "\n",
        "For example, the 2x2 identity matrix is:\n",
        "\n",
        "\n",
        "I = [[1, 0],\n",
        "     [0, 1]]\n",
        "When this matrix is multiplied by any 2x2 matrix A, the result is simply A:\n",
        "\n",
        "\n",
        "I * A = A * I = A"
      ],
      "metadata": {
        "id": "fda4o3Y3jI-8"
      }
    }
  ]
}