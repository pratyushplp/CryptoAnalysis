{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('final').getOrCreate()\n",
    "\n",
    "#The columns are in capital and small so we have to make it case, sensetive. By default its off.\n",
    "spark.conf.set('spark.sql.caseSensitive', True)\n",
    "\n",
    "file_path = '/Users/pratyushpradhan/Developer/Personal/Projects/Crypto/todayData'\n",
    "df = spark.read.option('header','true').csv(file_path, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO:  6 perform analytics task thorugh filter and search group by columns\n",
    "\n",
    "#TRANSFORMATION\n",
    "\n",
    "#drop columns\n",
    "df= df.drop('B','x','f','L')\n",
    "\n",
    "#rename\n",
    "col_list=['start_time','close_time','symbol','interval','open','close','high','low','base_volume','num_trades'\n",
    ",'quote_volume','taker_buy_base_volume','taker_buy_quote_volume']\n",
    "df = df.toDF(*col_list)\n",
    "\n",
    "#drop column if value null\n",
    "df= df.na.drop(how='any', subset=['symbol','start_time','close_time','open','close','high','low','base_volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update date values from unix epoch timestamp to timestamp\n",
    "#NOTE Unix epoch time is utc by default\n",
    "df = df.withColumn('start_time', f.to_timestamp(df.start_time/1000)).withColumn('close_time', f.to_timestamp(df.close_time/1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get distinct symbols from all columns\n",
    "dist_symbols = df.select('symbol').distinct().rdd.flatMap(lambda x:x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    }
   ],
   "source": [
    "#Convert spark dataframe to pandas\n",
    "##df.show()\n",
    "df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/f2w97nw92131s9_c7z1n0p900000gn/T/ipykernel_2166/1818204510.py:3: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  agg_df=df.groupby(['symbol','close_date']).sum().reset_index().copy()\n"
     ]
    }
   ],
   "source": [
    "#create aggregate Datatable\n",
    "df['close_date'] = (df['close_time']).dt.date\n",
    "agg_df=df.groupby(['symbol','close_date']).sum().reset_index().copy()\n",
    "agg_df=agg_df.rename(columns={'num_trades':'total_num_trades','base_volume': 'total_base_volume', 'quote_volume': 'total_quote_volume'})\n",
    "agg_df=agg_df.loc[:,['symbol','close_date','total_num_trades','total_base_volume','total_quote_volume']]\n",
    "\n",
    "#take only certain columns and rename them\n",
    "#droping the temp column\n",
    "df = df.drop('close_date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createConnection(database,user,password,host,port):\n",
    "    return create_engine(f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}?client_encoding=utf8\")\n",
    "\n",
    "#load data to database\n",
    "#TODO: CREATE CONFIG fiLE\n",
    "database='crypto'\n",
    "user='cryptouser'\n",
    "password= 'secretcrypto' \n",
    "host='127.0.0.1'\n",
    "port= '5432'\n",
    "\n",
    "#TODO: implement try catch  and logging\n",
    "engine = createConnection(database,user,password,host,port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ETHUSDT']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/06 19:06:03 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 1038830 ms exceeds timeout 120000 ms\n",
      "22/12/06 19:06:04 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "print(dist_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to insert to database\n",
    "\n",
    "for value in dist_symbols:\n",
    "    #note that the data table name is same as symbol name\n",
    "    # TODO: if time add a query so that if new symbol comes and its corresponding datatable does not exist, create new table with a predefined schema\n",
    "    #NOTE: always  \" \" inside filter at start function no ''\n",
    "    #NOTE: for postgres take db in lowercase, we need to add  quotes (\" \") always if taken to uppercase\n",
    "    temp_df = df.query(f\"symbol == '{value}' \").to_sql(value.lower(), engine, index=False, if_exists= 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert aggregate database to postgres\n",
    "table_name= 'aggregate_trade'\n",
    "display(agg_df)\n",
    "temp = agg_df.to_sql(table_name, engine, index=False, if_exists= 'append') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
