{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pratyushpradhan/Developer/Personal/Projects/Crypto/CryptoDataProcessingAndML/Version2/DataProcessing.py:34: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  MACD = ta.macd(df['close'])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ETL' object has no attribute 'create_tensor_with_sequence'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m tensor_temp\u001b[39m=\u001b[39m df[x_cols\u001b[39m+\u001b[39mlabels]\n\u001b[1;32m     31\u001b[0m sequence_num\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m\u001b[39m# also called timesteps\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m tensor \u001b[39m=\u001b[39m dataProcess\u001b[39m.\u001b[39;49mcreate_tensor_with_sequence(tensor_temp,labels,sequence_num)\n\u001b[1;32m     33\u001b[0m \u001b[39m#note we have 2 labels, one for daily(B) another for shortterm (A) => shorterm>daily\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m#y1= df['result_A']\u001b[39;00m\n\u001b[1;32m     35\u001b[0m X\u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39mloc[:,\u001b[39m~\u001b[39mtensor\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39misin(labels)]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ETL' object has no attribute 'create_tensor_with_sequence'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "import logging\n",
    "import DataProcessing\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation, Flatten, TimeDistributed, RepeatVector, Permute, Multiply, Lambda,Bidirectional\n",
    "from keras import backend as K\n",
    "\n",
    "dataProcess = DataProcessing.ETL()\n",
    "df = dataProcess.Load_Clean_Data('./historical_data/Binance_ETHUSDT_d.csv','1d')\n",
    "\n",
    "cols= ['ti_stoch_kd','ti_MACD']\n",
    "cols_to_normalize = ['open','high', 'low', 'close', 'base_volume'] # removed num_trades\n",
    "df = dataProcess.Add_TI_Data(df, cols=cols)\n",
    "df= dataProcess.Add_Label(df,cols_to_normalize=cols_to_normalize)\n",
    "#important: reverse the order and Reset the index to start from 0\n",
    "df= df[::-1]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#removed time ('unix_start_datetime')\n",
    "x_cols = cols_to_normalize+cols\n",
    "labels = ['result_A','result_B']\n",
    "tensor_temp= df[x_cols+labels]\n",
    "sequence_num=30# also called timesteps\n",
    "tensor = dataProcess.create_tensor_with_sequence(tensor_temp,labels,sequence_num)\n",
    "#note we have 2 labels, one for daily(B) another for shortterm (A) => shorterm>daily\n",
    "#y1= df['result_A']\n",
    "X= tensor.loc[:,~tensor.columns.isin(labels)]\n",
    "y2= tensor['result_B']\n",
    "y = y2.replace({'up': 1, 'down': -1, 'flat':0})\n",
    "print(X.shape)\n",
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "# further split training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "\n",
    "# print the shape of each set\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Validation data shape:\", X_val.shape)\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "\n",
    "#Important: reshape it into a 3D tensor with dimensions (number of samples, sequence length, number of features)\n",
    "#to process with LSTM\n",
    "X_train = X_train.reshape(X_train.shape[0],sequence_num,len(x_cols))\n",
    "X_test = X_test.reshape(X_test.shape[0],sequence_num,len(x_cols))\n",
    "X_val = X_val.reshape(X_val.shape[0],sequence_num,len(x_cols))\n",
    "\n",
    "\n",
    "\n",
    "# seq_model = Sequential()\n",
    "# # Add LSTM layer with return_sequences=True\n",
    "# seq_model.add(Bidirectional(LSTM(units=128, return_sequences=True, input_shape=(1291, 10)))\n",
    "# # Add attention mechanism\n",
    "# seq_model.add(Dense(units=1, activation='tanh'))\n",
    "# seq_model.add(Flatten())\n",
    "# seq_model.add(Activation('softmax'))\n",
    "# seq_model.add(RepeatVector(128))\n",
    "# seq_model.add(Permute([2, 1]))\n",
    "# seq_model.add(Multiply())\n",
    "# seq_model.add(Lambda(lambda x: K.sum(x, axis=1)))\n",
    "# # Add output layer\n",
    "# seq_model.add(Dense(1, activation='sigmoid'))\n",
    "# seq_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# seq_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5)\n",
    "# print('Sequential model performance:')\n",
    "# #NOTE: We can add an embedding layer in another implementation\n",
    "# loss, accuracy = seq_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "temp1 = ['1','2']\n",
    "temp2 = ['2','4']\n",
    "for val in temp1:\n",
    "    if(val in temp2):\n",
    "        print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>base_volume</th>\n",
       "      <th>ti_stoch_kd</th>\n",
       "      <th>ti_MACD</th>\n",
       "      <th>open_lag1</th>\n",
       "      <th>high_lag1</th>\n",
       "      <th>low_lag1</th>\n",
       "      <th>...</th>\n",
       "      <th>base_volume_lag13</th>\n",
       "      <th>ti_stoch_kd_lag13</th>\n",
       "      <th>ti_MACD_lag13</th>\n",
       "      <th>open_lag14</th>\n",
       "      <th>high_lag14</th>\n",
       "      <th>low_lag14</th>\n",
       "      <th>close_lag14</th>\n",
       "      <th>base_volume_lag14</th>\n",
       "      <th>ti_stoch_kd_lag14</th>\n",
       "      <th>ti_MACD_lag14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.217258</td>\n",
       "      <td>0.222403</td>\n",
       "      <td>0.212381</td>\n",
       "      <td>0.217385</td>\n",
       "      <td>0.123336</td>\n",
       "      <td>0.058882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217123</td>\n",
       "      <td>0.222270</td>\n",
       "      <td>0.212250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122619</td>\n",
       "      <td>0.057884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.215338</td>\n",
       "      <td>0.220471</td>\n",
       "      <td>0.210446</td>\n",
       "      <td>0.215464</td>\n",
       "      <td>0.122589</td>\n",
       "      <td>0.057884</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.246188</td>\n",
       "      <td>0.250897</td>\n",
       "      <td>0.241633</td>\n",
       "      <td>0.246180</td>\n",
       "      <td>0.108500</td>\n",
       "      <td>0.737367</td>\n",
       "      <td>0.282631</td>\n",
       "      <td>0.246199</td>\n",
       "      <td>0.250910</td>\n",
       "      <td>0.241645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108911</td>\n",
       "      <td>0.736769</td>\n",
       "      <td>0.282631</td>\n",
       "      <td>0.246410</td>\n",
       "      <td>0.251131</td>\n",
       "      <td>0.241836</td>\n",
       "      <td>0.246393</td>\n",
       "      <td>0.108936</td>\n",
       "      <td>0.736769</td>\n",
       "      <td>0.282631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.028409</td>\n",
       "      <td>0.029260</td>\n",
       "      <td>0.027679</td>\n",
       "      <td>0.028410</td>\n",
       "      <td>0.050881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028409</td>\n",
       "      <td>0.029260</td>\n",
       "      <td>0.027679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028409</td>\n",
       "      <td>0.029260</td>\n",
       "      <td>0.027665</td>\n",
       "      <td>0.028410</td>\n",
       "      <td>0.049636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.085360</td>\n",
       "      <td>0.089525</td>\n",
       "      <td>0.082821</td>\n",
       "      <td>0.086042</td>\n",
       "      <td>0.096814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085159</td>\n",
       "      <td>0.089275</td>\n",
       "      <td>0.082745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082228</td>\n",
       "      <td>0.084039</td>\n",
       "      <td>0.080959</td>\n",
       "      <td>0.082363</td>\n",
       "      <td>0.096182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.336886</td>\n",
       "      <td>0.343068</td>\n",
       "      <td>0.334052</td>\n",
       "      <td>0.336885</td>\n",
       "      <td>0.164224</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.336886</td>\n",
       "      <td>0.343068</td>\n",
       "      <td>0.334052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334679</td>\n",
       "      <td>0.342124</td>\n",
       "      <td>0.329251</td>\n",
       "      <td>0.334650</td>\n",
       "      <td>0.163941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              open         high          low        close  base_volume  \\\n",
       "count  2004.000000  2004.000000  2004.000000  2004.000000  2004.000000   \n",
       "mean      0.217258     0.222403     0.212381     0.217385     0.123336   \n",
       "std       0.246188     0.250897     0.241633     0.246180     0.108500   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.028409     0.029260     0.027679     0.028410     0.050881   \n",
       "50%       0.085360     0.089525     0.082821     0.086042     0.096814   \n",
       "75%       0.336886     0.343068     0.334052     0.336885     0.164224   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       ti_stoch_kd      ti_MACD    open_lag1    high_lag1     low_lag1  ...  \\\n",
       "count  2004.000000  2004.000000  2004.000000  2004.000000  2004.000000  ...   \n",
       "mean      0.058882     0.000000     0.217123     0.222270     0.212250  ...   \n",
       "std       0.737367     0.282631     0.246199     0.250910     0.241645  ...   \n",
       "min      -1.000000    -1.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     0.000000     0.028409     0.029260     0.027679  ...   \n",
       "50%       0.000000     0.000000     0.085159     0.089275     0.082745  ...   \n",
       "75%       1.000000     0.000000     0.336886     0.343068     0.334052  ...   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
       "\n",
       "       base_volume_lag13  ti_stoch_kd_lag13  ti_MACD_lag13   open_lag14  \\\n",
       "count        2004.000000        2004.000000    2004.000000  2004.000000   \n",
       "mean            0.122619           0.057884       0.000000     0.215338   \n",
       "std             0.108911           0.736769       0.282631     0.246410   \n",
       "min             0.000000          -1.000000      -1.000000     0.000000   \n",
       "25%             0.049663           0.000000       0.000000     0.028409   \n",
       "50%             0.096182           0.000000       0.000000     0.082228   \n",
       "75%             0.163941           1.000000       0.000000     0.334679   \n",
       "max             1.000000           1.000000       1.000000     1.000000   \n",
       "\n",
       "        high_lag14    low_lag14  close_lag14  base_volume_lag14  \\\n",
       "count  2004.000000  2004.000000  2004.000000        2004.000000   \n",
       "mean      0.220471     0.210446     0.215464           0.122589   \n",
       "std       0.251131     0.241836     0.246393           0.108936   \n",
       "min       0.000000     0.000000     0.000000           0.000000   \n",
       "25%       0.029260     0.027665     0.028410           0.049636   \n",
       "50%       0.084039     0.080959     0.082363           0.096182   \n",
       "75%       0.342124     0.329251     0.334650           0.163941   \n",
       "max       1.000000     1.000000     1.000000           1.000000   \n",
       "\n",
       "       ti_stoch_kd_lag14  ti_MACD_lag14  \n",
       "count        2004.000000    2004.000000  \n",
       "mean            0.057884       0.000000  \n",
       "std             0.736769       0.282631  \n",
       "min            -1.000000      -1.000000  \n",
       "25%             0.000000       0.000000  \n",
       "50%             0.000000       0.000000  \n",
       "75%             1.000000       0.000000  \n",
       "max             1.000000       1.000000  \n",
       "\n",
       "[8 rows x 105 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#code to convert to 3d vector\n",
    "# Define sequence length\n",
    "sequence_length = 14\n",
    "# Create a copy of the original data\n",
    "data_seq = X.copy()\n",
    "# Create new columns for each day of historical data\n",
    "for i in range(1, sequence_length + 1):\n",
    "    for col in X.columns:\n",
    "        data_seq[f'{col}_lag{i}'] = X[col].shift(i)\n",
    "\n",
    "# Drop rows with missing values\n",
    "data_seq.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df.head(10))\n",
    "# train test split\n",
    "#note we have 2 labels, one for daily(B) another for shortterm (A) => shorterm>daily\n",
    "#taking daily B 'result_B'\n",
    "col_label='result_B'\n",
    "training_size = int(len(df)*70)\n",
    "test_size = len(df)-training_size\n",
    "X_train, X_test, y_train, y_test= df[0:training_size,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>base_volume</th>\n",
       "      <th>quote_volume</th>\n",
       "      <th>num_trades</th>\n",
       "      <th>ti_stoch_kd</th>\n",
       "      <th>ti_MACD</th>\n",
       "      <th>unix_start_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>0.034035</td>\n",
       "      <td>0.033703</td>\n",
       "      <td>0.030982</td>\n",
       "      <td>0.030545</td>\n",
       "      <td>0.074875</td>\n",
       "      <td>0.007061</td>\n",
       "      <td>0.021772</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1537747200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>0.033288</td>\n",
       "      <td>0.034441</td>\n",
       "      <td>0.033507</td>\n",
       "      <td>0.034025</td>\n",
       "      <td>0.050224</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.015403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1537660800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>0.034698</td>\n",
       "      <td>0.035570</td>\n",
       "      <td>0.032579</td>\n",
       "      <td>0.033286</td>\n",
       "      <td>0.090661</td>\n",
       "      <td>0.008780</td>\n",
       "      <td>0.022729</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1537574400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>0.029687</td>\n",
       "      <td>0.034938</td>\n",
       "      <td>0.029980</td>\n",
       "      <td>0.034708</td>\n",
       "      <td>0.180167</td>\n",
       "      <td>0.016744</td>\n",
       "      <td>0.046171</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1537488000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>0.026679</td>\n",
       "      <td>0.029622</td>\n",
       "      <td>0.026720</td>\n",
       "      <td>0.029685</td>\n",
       "      <td>0.122812</td>\n",
       "      <td>0.010429</td>\n",
       "      <td>0.030893</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1537401600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>0.026389</td>\n",
       "      <td>0.027146</td>\n",
       "      <td>0.024997</td>\n",
       "      <td>0.026677</td>\n",
       "      <td>0.106182</td>\n",
       "      <td>0.008833</td>\n",
       "      <td>0.022954</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1537315200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>0.023813</td>\n",
       "      <td>0.027278</td>\n",
       "      <td>0.024110</td>\n",
       "      <td>0.026335</td>\n",
       "      <td>0.125114</td>\n",
       "      <td>0.010177</td>\n",
       "      <td>0.029854</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1537228800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>0.028891</td>\n",
       "      <td>0.029270</td>\n",
       "      <td>0.023879</td>\n",
       "      <td>0.023792</td>\n",
       "      <td>0.176733</td>\n",
       "      <td>0.014837</td>\n",
       "      <td>0.040152</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1537142400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>0.029184</td>\n",
       "      <td>0.028957</td>\n",
       "      <td>0.027268</td>\n",
       "      <td>0.028875</td>\n",
       "      <td>0.091047</td>\n",
       "      <td>0.007909</td>\n",
       "      <td>0.023336</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1537056000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>0.026417</td>\n",
       "      <td>0.029776</td>\n",
       "      <td>0.027169</td>\n",
       "      <td>0.029112</td>\n",
       "      <td>0.094373</td>\n",
       "      <td>0.008272</td>\n",
       "      <td>0.022332</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1536969600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657</th>\n",
       "      <td>0.026917</td>\n",
       "      <td>0.029103</td>\n",
       "      <td>0.026081</td>\n",
       "      <td>0.026472</td>\n",
       "      <td>0.178647</td>\n",
       "      <td>0.015231</td>\n",
       "      <td>0.044798</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1536883200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1658</th>\n",
       "      <td>0.021045</td>\n",
       "      <td>0.027180</td>\n",
       "      <td>0.021882</td>\n",
       "      <td>0.026866</td>\n",
       "      <td>0.158390</td>\n",
       "      <td>0.012626</td>\n",
       "      <td>0.039783</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1536796800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1659</th>\n",
       "      <td>0.021462</td>\n",
       "      <td>0.020953</td>\n",
       "      <td>0.018465</td>\n",
       "      <td>0.021032</td>\n",
       "      <td>0.107887</td>\n",
       "      <td>0.007566</td>\n",
       "      <td>0.026931</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1536710400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>0.024019</td>\n",
       "      <td>0.024085</td>\n",
       "      <td>0.020373</td>\n",
       "      <td>0.021468</td>\n",
       "      <td>0.102031</td>\n",
       "      <td>0.007698</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1536624000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>0.023788</td>\n",
       "      <td>0.024759</td>\n",
       "      <td>0.022735</td>\n",
       "      <td>0.024002</td>\n",
       "      <td>0.089535</td>\n",
       "      <td>0.007014</td>\n",
       "      <td>0.026189</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1536537600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>0.023945</td>\n",
       "      <td>0.025842</td>\n",
       "      <td>0.022286</td>\n",
       "      <td>0.023718</td>\n",
       "      <td>0.091844</td>\n",
       "      <td>0.007264</td>\n",
       "      <td>0.025844</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1536451200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>0.027823</td>\n",
       "      <td>0.028267</td>\n",
       "      <td>0.023363</td>\n",
       "      <td>0.023909</td>\n",
       "      <td>0.068165</td>\n",
       "      <td>0.005591</td>\n",
       "      <td>0.019911</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1536364800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.031296</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.027827</td>\n",
       "      <td>0.085222</td>\n",
       "      <td>0.007591</td>\n",
       "      <td>0.029022</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1536278400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>0.030593</td>\n",
       "      <td>0.030983</td>\n",
       "      <td>0.028110</td>\n",
       "      <td>0.030892</td>\n",
       "      <td>0.118386</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.043215</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1536192000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>0.042638</td>\n",
       "      <td>0.042363</td>\n",
       "      <td>0.031349</td>\n",
       "      <td>0.030644</td>\n",
       "      <td>0.113182</td>\n",
       "      <td>0.011710</td>\n",
       "      <td>0.039083</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1536105600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          open      high       low     close  base_volume  quote_volume  \\\n",
       "1647  0.034035  0.033703  0.030982  0.030545     0.074875      0.007061   \n",
       "1648  0.033288  0.034441  0.033507  0.034025     0.050224      0.004878   \n",
       "1649  0.034698  0.035570  0.032579  0.033286     0.090661      0.008780   \n",
       "1650  0.029687  0.034938  0.029980  0.034708     0.180167      0.016744   \n",
       "1651  0.026679  0.029622  0.026720  0.029685     0.122812      0.010429   \n",
       "1652  0.026389  0.027146  0.024997  0.026677     0.106182      0.008833   \n",
       "1653  0.023813  0.027278  0.024110  0.026335     0.125114      0.010177   \n",
       "1654  0.028891  0.029270  0.023879  0.023792     0.176733      0.014837   \n",
       "1655  0.029184  0.028957  0.027268  0.028875     0.091047      0.007909   \n",
       "1656  0.026417  0.029776  0.027169  0.029112     0.094373      0.008272   \n",
       "1657  0.026917  0.029103  0.026081  0.026472     0.178647      0.015231   \n",
       "1658  0.021045  0.027180  0.021882  0.026866     0.158390      0.012626   \n",
       "1659  0.021462  0.020953  0.018465  0.021032     0.107887      0.007566   \n",
       "1660  0.024019  0.024085  0.020373  0.021468     0.102031      0.007698   \n",
       "1661  0.023788  0.024759  0.022735  0.024002     0.089535      0.007014   \n",
       "1662  0.023945  0.025842  0.022286  0.023718     0.091844      0.007264   \n",
       "1663  0.027823  0.028267  0.023363  0.023909     0.068165      0.005591   \n",
       "1664  0.030900  0.031296  0.028434  0.027827     0.085222      0.007591   \n",
       "1665  0.030593  0.030983  0.028110  0.030892     0.118386      0.010700   \n",
       "1666  0.042638  0.042363  0.031349  0.030644     0.113182      0.011710   \n",
       "\n",
       "      num_trades  ti_stoch_kd  ti_MACD  unix_start_datetime  \n",
       "1647    0.021772            1        0        1537747200000  \n",
       "1648    0.015403            0        0        1537660800000  \n",
       "1649    0.022729           -1        1        1537574400000  \n",
       "1650    0.046171           -1        0        1537488000000  \n",
       "1651    0.030893            0       -1        1537401600000  \n",
       "1652    0.022954            1        0        1537315200000  \n",
       "1653    0.029854            1        0        1537228800000  \n",
       "1654    0.040152            1        0        1537142400000  \n",
       "1655    0.023336            0        0        1537056000000  \n",
       "1656    0.022332           -1        0        1536969600000  \n",
       "1657    0.044798           -1        0        1536883200000  \n",
       "1658    0.039783           -1        0        1536796800000  \n",
       "1659    0.026931            0        0        1536710400000  \n",
       "1660    0.029297            1        0        1536624000000  \n",
       "1661    0.026189            1        0        1536537600000  \n",
       "1662    0.025844            0        0        1536451200000  \n",
       "1663    0.019911           -1        0        1536364800000  \n",
       "1664    0.029022           -1        0        1536278400000  \n",
       "1665    0.043215           -1        1        1536192000000  \n",
       "1666    0.039083           -1        0        1536105600000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_test.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = Sequential()\n",
    "# Add LSTM layer with return_sequences=True\n",
    "seq_model.add(Bidirectional(LSTM(units=128, return_sequences=True, input_shape=(1291, 10))))\n",
    "# Add attention mechanism\n",
    "# seq_model.add(Dense(units=1, activation='tanh'))\n",
    "# seq_model.add(Flatten())\n",
    "# seq_model.add(Activation('softmax'))\n",
    "# seq_model.add(RepeatVector(128))\n",
    "# seq_model.add(Permute([2, 1]))\n",
    "# seq_model.add(Multiply())\n",
    "# seq_model.add(Lambda(lambda x: K.sum(x, axis=1)))\n",
    "# Add output layer\n",
    "seq_model.add(Dense(1, activation='sigmoid'))\n",
    "seq_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "seq_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5)\n",
    "print('Sequential model performance:')\n",
    "#NOTE: We can add an embedding layer in another implementation\n",
    "loss, accuracy = seq_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "import logging\n",
    "import DataProcessing\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation, Flatten, TimeDistributed, RepeatVector, Permute, Multiply, Lambda,Bidirectional\n",
    "from keras import backend as K\n",
    "\n",
    "dataProcess = DataProcessing.ETL()\n",
    "df = dataProcess.Load_Clean_Data('./historical_data/Binance_ETHUSDT_d.csv','1d')\n",
    "\n",
    "cols_ti= ['ti_stoch_kd','ti_MACD']\n",
    "cols_to_normalize = ['open','high', 'low', 'close', 'base_volume', 'quote_volume', 'num_trades']\n",
    "\n",
    "df = dataProcess.Add_TI_Data(df, cols=cols_ti)\n",
    "df = dataProcess.Add_Label(df,cols_to_normalize=cols_to_normalize)\n",
    "\n",
    "#Note we are not taking time data (unix_start_datetime)\n",
    "x_cols = cols_to_normalize+cols_ti\n",
    "labels = ['result_A','result_B']\n",
    "\n",
    "X= df[x_cols]\n",
    "#note we have 2 labels, one for daily(B) another for shortterm (A) => shorterm>daily\n",
    "y1= df['result_A']\n",
    "y2= df['result_B']\n",
    "\n",
    "\n",
    "y = y2.replace({'up': 1, 'down': -1, 'flat':0})\n",
    "print(y)\n",
    "y = to_categorical(y)\n",
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# further split training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "\n",
    "# print the shape of each set\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Validation data shape:\", X_val.shape)\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "\n",
    "\n",
    "seq_model = Sequential()\n",
    "# Add LSTM layer with return_sequences=True\n",
    "seq_model.add(Bidirectional(LSTM(units=128, return_sequences=True, input_shape=(1291, 10))))\n",
    "# Add attention mechanism\n",
    "seq_model.add(Dense(units=1, activation='tanh'))\n",
    "seq_model.add(Flatten())\n",
    "seq_model.add(Activation('softmax'))\n",
    "seq_model.add(RepeatVector(128))\n",
    "seq_model.add(Permute([2, 1]))\n",
    "seq_model.add(Multiply())\n",
    "seq_model.add(Lambda(lambda x: K.sum(x, axis=1)))\n",
    "# Add output layer\n",
    "seq_model.add(Dense(3, activation='softmax'))\n",
    "seq_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "seq_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5)\n",
    "print('Sequential model performance:')\n",
    "#NOTE: We can add an embedding layer in another implementation\n",
    "loss, accuracy = seq_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to use softmax as the last layer for classifying up down flat along with the probability\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a simple neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(3, input_dim=4, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Generate some input data\n",
    "X = np.array([[1, 2, 3, 4]])\n",
    "\n",
    "# Get the predicted probabilities for each class\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Get the predicted class label and associated probability\n",
    "predicted_class_label = np.argmax(y_pred)\n",
    "predicted_probability = y_pred[0][predicted_class_label]\n",
    "\n",
    "# Print the results\n",
    "print('Predicted class label:', predicted_class_label)\n",
    "print('Associated probability:', predicted_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to convert to 3d vector\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define sequence length\n",
    "sequence_length = 30\n",
    "\n",
    "# Create a copy of the original data\n",
    "data_seq = stock_data.copy()\n",
    "\n",
    "# Create new columns for each day of historical data\n",
    "for i in range(1, sequence_length + 1):\n",
    "    for col in stock_data.columns:\n",
    "        data_seq[f'{col}_lag{i}'] = stock_data[col].shift(i)\n",
    "\n",
    "# Drop rows with missing values\n",
    "data_seq.dropna(inplace=True)\n",
    "\n",
    "# Get X and y\n",
    "X = data_seq.drop(['target_column'], axis=1)\n",
    "y = data_seq['target_column']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
